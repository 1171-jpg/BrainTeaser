{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from overrides import overrides\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import DebertaV2ForMaskedLM\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/deberta-v3-large'\n",
    "cache_dir = '../Model/best_model'\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = DebertaV2ForMaskedLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data_path = \"\"\n",
    "wordplay_data_list = \"\"\n",
    "sentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\n",
    "wordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_list = sentence_data_list + wordplay_data_list\n",
    "test_data_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "predictions = []\n",
    "results = []\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_data_list[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_PER_TIME = 80\n",
    "choice_list= ['A','B','C','D']\n",
    "for sample in tqdm(test_data_list):\n",
    "    predict = score_task(sample['question'],sample['choice_list'],tokenizer, device, model)\n",
    "    sample['predict'] = choice_list[int(predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_play,reverse_play = getResultdata(test_data_list)\n",
    "final_result = getSeperateResult(word_play,reverse_play)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_task(question, choices, tokenizer, device, model):\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    question_ids = tokenizer.encode(question)\n",
    "    choice_ids = [tokenizer.encode(choice, add_prefix_space=True)[1:-1] for choice in choices]\n",
    "    sequences = [question_ids[:-1] + choice_ids[i] + [tokenizer.sep_token_id] for i in range(len(choice_ids))]\n",
    "    label_ids = [[-100] + text[1:-1] + [-100] for text in sequences]\n",
    "    sequences, label_ids, attention_mask = prepare_input(sequences, label_ids, pad_token_id)\n",
    "    prediction = token_wise_scoring(sequences, label_ids, attention_mask, tokenizer, device, model)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_wise_scoring(sequences, label_ids, attention_mask, tokenizer, device, model):\n",
    "    choice_loss = [0 for i in range(len(sequences))]\n",
    "    for i in range(len(sequences)):\n",
    "        tmp_seq_list = []\n",
    "        tmp_label_list = []\n",
    "        tmp_attention_mask = []\n",
    "        curr_label_ids = label_ids[i]\n",
    "        for j, t in enumerate(curr_label_ids):\n",
    "            if t == -100:\n",
    "                continue\n",
    "            tmp_seq = torch.tensor(sequences[i][:j] + [tokenizer.mask_token_id] + sequences[i][j + 1:]).long().to(\n",
    "                device)\n",
    "            tmp_label = torch.tensor(\n",
    "                [-100] * j + sequences[i][j:j + 1] + [-100] * (len(sequences[i]) - j - 1)).long().to(device)\n",
    "            tmp_seq_list.append(tmp_seq)\n",
    "            tmp_label_list.append(tmp_label)\n",
    "            tmp_attention_mask.append(torch.tensor(attention_mask[i]).long().to(device))\n",
    "        tmp_seq_list = torch.stack(tmp_seq_list)\n",
    "        tmp_label_list = torch.stack(tmp_label_list)\n",
    "        tmp_attention_mask = torch.stack(tmp_attention_mask)\n",
    "        if len(tmp_seq_list) < MAX_SEQUENCE_PER_TIME:\n",
    "            loss = get_lm_score(model, tmp_seq_list, tmp_label_list, tmp_attention_mask)\n",
    "        else:\n",
    "            loss = []\n",
    "            for chunk in range(0, len(tmp_seq_list), MAX_SEQUENCE_PER_TIME):\n",
    "                loss.append(get_lm_score(model, tmp_seq_list[chunk:chunk + MAX_SEQUENCE_PER_TIME],\n",
    "                                         tmp_label_list[chunk:chunk + MAX_SEQUENCE_PER_TIME],\n",
    "                                         tmp_attention_mask[chunk:chunk + MAX_SEQUENCE_PER_TIME]))\n",
    "            loss = np.concatenate(loss)\n",
    "        choice_loss[i] = sum(loss) / len(loss)\n",
    "    prediction = choice_loss.index(min(choice_loss))\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def prepare_input(sequences, label_ids, pad_token_id):\n",
    "    max_length = max([len(text) for text in sequences])\n",
    "    attention_mask = np.zeros((len(sequences), max_length))\n",
    "    for i in range(len(sequences)):\n",
    "        attention_mask[i][:len(sequences[i])] = 1\n",
    "    sequences = [text + [pad_token_id] * (max_length - len(text)) for text in sequences]\n",
    "    label_ids = [text + [-100] * (max_length - len(text)) for text in label_ids]\n",
    "    return sequences, label_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm_score(model, batch, label_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Get the cross entropy loss of the texts in batch using the langage model\n",
    "    \"\"\"\n",
    "    # Batch: [num_choices, max_length]\n",
    "    with torch.no_grad():\n",
    "        num_choices, max_length = batch.shape\n",
    "        label_ids = label_ids.view(-1)\n",
    "        lm_logits = model(batch, attention_mask=attention_mask)[0]\n",
    "        lm_logits = lm_logits.view(-1, lm_logits.size(-1))\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "        loss = loss_fct(lm_logits, label_ids)\n",
    "        loss = loss.view(num_choices, -1).sum(1).cpu().numpy()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tripPy_copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
