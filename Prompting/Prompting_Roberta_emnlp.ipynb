{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from overrides import overrides\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = \"roberta-large\"\n",
    "device=\"cuda:0\"\n",
    "# lm = '../Model/roberta_cskg/'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(lm)\n",
    "model = RobertaForMaskedLM.from_pretrained(lm)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886384df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data_path = \"\"\n",
    "wordplay_data_list = \"\"\n",
    "sentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\n",
    "wordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_list = sentence_data_list + wordplay_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75071074",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_PER_TIME= 80\n",
    "gold = []\n",
    "predictions = []\n",
    "results = []\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "\n",
    "for sample_data in tqdm(test_data_list):\n",
    "    question = sample_data['question']\n",
    "    label = sample_data['label']\n",
    "    choices = [sub_answer[0].lower()+sub_answer[1:] for sub_answer in sample_data['choice_list'] ]\n",
    "    gold.append(label)\n",
    "    prediction = score_task(question, choices, tokenizer, device, model)\n",
    "    sample_data['predict'] = prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_to_choine = {0:'A',1:'B',2:'C',3:'D'}\n",
    "for item in test_data_list:\n",
    "    item['predict'] = predict_to_choine[item['predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90967323",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_play,sentence_play = getResultdata(test_data_list)\n",
    "final_result = getSeperateResult(word_play,sentence_play)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adb99a94",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac68b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Roberta#######################\n",
    "def score_task(question, choices, tokenizer, device, model):\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    question_ids = tokenizer.encode(question)\n",
    "    choice_ids = [tokenizer.encode(choice, add_prefix_space=True)[1:-1] for choice in choices]\n",
    "    sequences = [question_ids[:-1] + choice_ids[i] +[tokenizer.sep_token_id] for i in range(len(choice_ids))]\n",
    "    label_ids = [[-100]+text[1:-1]+[-100] for text in sequences]\n",
    "    sequences, label_ids, attention_mask = prepare_input(sequences, label_ids, pad_token_id)\n",
    "    prediction = token_wise_scoring(sequences, label_ids, attention_mask, tokenizer, device, model)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def prepare_input(sequences, label_ids, pad_token_id):\n",
    "    max_length = max([len(text) for text in sequences])\n",
    "    attention_mask = np.zeros((len(sequences), max_length))\n",
    "    for i in range(len(sequences)):\n",
    "        attention_mask[i][:len(sequences[i])] = 1\n",
    "    sequences = [text + [pad_token_id] * (max_length - len(text)) for text in sequences]\n",
    "    label_ids = [text + [-100] * (max_length - len(text)) for text in label_ids]\n",
    "    return sequences, label_ids, attention_mask\n",
    "\n",
    "\n",
    "def token_wise_scoring(sequences, label_ids, attention_mask, tokenizer, device, model):\n",
    "    choice_loss = [0 for i in range(len(sequences))]\n",
    "    for i in range(len(sequences)):\n",
    "        tmp_seq_list = []\n",
    "        tmp_label_list = []\n",
    "        tmp_attention_mask = []\n",
    "        curr_label_ids = label_ids[i]\n",
    "        for j, t in enumerate(curr_label_ids):\n",
    "            if t == -100:\n",
    "                continue\n",
    "            tmp_seq = torch.tensor(sequences[i][:j]+[tokenizer.mask_token_id]+sequences[i][j+1:]).long().to(device)\n",
    "            tmp_label = torch.tensor([-100]*j+sequences[i][j:j+1]+[-100]*(len(sequences[i])-j-1)).long().to(device)\n",
    "            tmp_seq_list.append(tmp_seq)\n",
    "            tmp_label_list.append(tmp_label)\n",
    "            tmp_attention_mask.append(torch.tensor(attention_mask[i]).long().to(device))\n",
    "        tmp_seq_list = torch.stack(tmp_seq_list)\n",
    "        tmp_label_list = torch.stack(tmp_label_list)\n",
    "        tmp_attention_mask = torch.stack(tmp_attention_mask)\n",
    "        if len(tmp_seq_list) < MAX_SEQUENCE_PER_TIME:\n",
    "            loss = get_lm_score(model, tmp_seq_list, tmp_label_list, tmp_attention_mask)\n",
    "        else:\n",
    "            loss = []\n",
    "            for chunk in range(0, len(tmp_seq_list), MAX_SEQUENCE_PER_TIME):\n",
    "                loss.append(get_lm_score(model, tmp_seq_list[chunk:chunk+MAX_SEQUENCE_PER_TIME], tmp_label_list[chunk:chunk+MAX_SEQUENCE_PER_TIME], tmp_attention_mask[chunk:chunk+MAX_SEQUENCE_PER_TIME]))\n",
    "            loss = np.concatenate(loss)\n",
    "        choice_loss[i] = sum(loss)/len(loss) \n",
    "    prediction = choice_loss.index(min(choice_loss))\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_lm_score(model, batch, label_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Get the cross entropy loss of the texts in batch using the langage model\n",
    "    \"\"\"\n",
    "    # Batch: [num_choices, max_length]\n",
    "    with torch.no_grad():\n",
    "        num_choices, max_length = batch.shape\n",
    "        label_ids = label_ids.view(-1)\n",
    "        lm_logits = model(batch, attention_mask=attention_mask)[0]\n",
    "        lm_logits = lm_logits.view(-1, lm_logits.size(-1))\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "        loss = loss_fct(lm_logits, label_ids)\n",
    "        loss = loss.view(num_choices, -1).sum(1).cpu().numpy()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc03a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
